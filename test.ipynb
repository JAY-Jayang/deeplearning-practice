{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "def cross_entropy_error(y, t, mode='label'):\r\n",
    "    if y.ndim == 1:\r\n",
    "        t = t.reshape(1, t.size)\r\n",
    "        y = y.reshape(1, y.size)\r\n",
    "\r\n",
    "    batch_size = y.shape[0]\r\n",
    "\r\n",
    "    if mode == 'label':\r\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t]+1e-7))/ batch_size\r\n",
    "    elif mode == 'one_hot':\r\n",
    "        # TODO: Fill out the return value below that calculates CEL \r\n",
    "        #       between one-hot encoded label 't' and the prediction 'y'.\r\n",
    "        return -np.sum()\r\n",
    "\r\n",
    "\r\n",
    "y_label = [2, 7, 0, 4, 9, 6, 5, 1, 3, 8]\r\n",
    "y_prob = [\r\n",
    "    [0.1, 0.05, 0.05, 0.59, 0.0, 0.05, 0.1, 0.0, 0.06, 0.0],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.6, 0.0, 0.0, 0.0],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.0, 0.0, 0.6],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.1, 0.05, 0.1, 0.35],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.3, 0.3, 0.0, 0.0],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.2, 0.4, 0.0, 0.0],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.2, 0.1, 0.1, 0.2],\r\n",
    "    [0.1, 0.05, 0.1, 0.4, 0.05, 0.1, 0.0, 0.1, 0.1, 0.0],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.3, 0.2, 0.1],\r\n",
    "    [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.1, 0.05, 0.1, 0.35],\r\n",
    "]\r\n",
    "\r\n",
    "\r\n",
    "y_label = np.array(y_label)\r\n",
    "y_prob = np.array(y_prob)\r\n",
    "\r\n",
    "batch_size = y_prob.shape[0]\r\n",
    "n_features = y_prob.shape[1]\r\n",
    "\r\n",
    "one_hot = np.zeros([batch_size, n_features])\r\n",
    "for i in range(batch_size):\r\n",
    "    one_hot[i, y_label[i]] = 1\r\n",
    "\r\n",
    "mini_batch_size = 5\r\n",
    "\r\n",
    "loss_history = []\r\n",
    "\r\n",
    "for n, i in enumerate(range(0,  y_prob.shape[0], mini_batch_size)):\r\n",
    "    y_label_mini = y_label[i:i+mini_batch_size+1]\r\n",
    "    y_prob_mini = y_prob[i:i+mini_batch_size+1]\r\n",
    "    \r\n",
    "    loss = cross_entropy_error(y_prob_mini, y_label_mini)\r\n",
    "    loss_history.append(loss)\r\n",
    "    print(f\"Loss of {n}-th batch: {loss:.4f}\")\r\n",
    "\r\n",
    "print(f\"Training finished ! Loss is {np.array(loss_history).mean():4f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss of 0-th batch: 7.0233\n",
      "Loss of 1-th batch: 5.0657\n",
      "Training finished ! Loss is 6.044483\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}